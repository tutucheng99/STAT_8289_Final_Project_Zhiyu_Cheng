{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Sampling for FDA Analysis\n",
    "\n",
    "This notebook collects policy behavior samples from π^H and π^R for downstream FDA analysis.\n",
    "\n",
    "**Target:** 1M samples\n",
    "**Estimated time:** ~20-30 min on A100\n",
    "\n",
    "## Output Format\n",
    "```\n",
    "policy_samples.npz\n",
    "├── observations        (N, 480)  bool     — Raw PGX observations (cast to float32 for inference)\n",
    "├── covariates          (N, 48)   float32  — 48 statistical features\n",
    "├── pi_H                (N, 38)   float32  — Smoothed π^H probabilities\n",
    "├── pi_R                (N, 38)   float32  — Smoothed π^R probabilities\n",
    "├── legal_masks         (N, 38)   bool     — Legal action masks\n",
    "├── episode_ids         (N,)      int32    — Episode index (for cluster bootstrap)\n",
    "├── board_ids           (N,)      int32    — Unique board index (without-replacement sampling)\n",
    "└── timestep_in_episode (N,)      int16    — Bidding round within episode\n",
    "\n",
    "metadata.json\n",
    "├── action_names        — 38 action names: [\"Pass\", \"Dbl\", \"Rdbl\", \"1C\", ..., \"7NT\"]\n",
    "├── ref_action          — ALR reference: \"Pass\"\n",
    "├── ref_action_idx      — ALR reference index: 0 (Pass is always action 0 in PGX)\n",
    "├── action_legal_rates  — Per-action legality rates\n",
    "├── rare_actions        — Actions with legal_rate < 0.5%\n",
    "└── states_per_episode  — {min, median, max, mean}\n",
    "```\n",
    "\n",
    "## Key Design Choices\n",
    "1. **Additive Smoothing**: `(p + ε) / (1 + K*ε)` ensures p > 0 for ALR transform\n",
    "2. **Without-replacement**: Uses `state._hand` fingerprint to avoid duplicate boards\n",
    "3. **Cluster Bootstrap**: Save `episode_ids` for non-i.i.d. inference in Step 4\n",
    "4. **Pass = action 0**: PGX encoding order (Pass, Dbl, Rdbl, 1C, ..., 7NT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "### ⚠️ IMPORTANT: Two-Phase Setup\n",
    "\n",
    "**Phase 1 (First time only):**\n",
    "1. Run cell 2-3 to install dependencies\n",
    "2. Restart runtime when prompted\n",
    "3. **DO NOT run cells 2-3 again after restart**\n",
    "\n",
    "**Phase 2 (After restart):**\n",
    "1. Skip cells 2-3\n",
    "2. Start from cell 4 and run all remaining cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up old JAX cuda plugin and install dependencies\n",
    "!pip uninstall -y jax-cuda12-plugin jax-cuda12-pjrt 2>/dev/null\n",
    "!pip install -q pgx==2.4.2 dm-haiku optax\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify JAX GPU\n",
    "import jax\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Devices: {jax.devices()}\")\n",
    "print(f\"Default backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone or copy project\n",
    "# Option 1: Clone from GitHub (if public)\n",
    "# !git clone https://github.com/your-repo/bridge_bidding_interpretability.git\n",
    "\n",
    "# Option 2: Copy from Drive\n",
    "# Adjust path as needed\n",
    "PROJECT_PATH = \"/content/drive/MyDrive/bridge_bidding_interpretability\"\n",
    "\n",
    "import os\n",
    "if os.path.exists(PROJECT_PATH):\n",
    "    %cd {PROJECT_PATH}\n",
    "    print(f\"Using project at: {PROJECT_PATH}\")\n",
    "else:\n",
    "    print(f\"Project not found at {PROJECT_PATH}\")\n",
    "    print(\"Please upload the project or adjust PROJECT_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project to path\n",
    "import sys\n",
    "sys.path.insert(0, PROJECT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Models & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pgx.bridge_bidding import BridgeBidding\n",
    "\n",
    "from src.policy_loader import PolicyWrapper\n",
    "from src.features.feature_extractor import BridgeFeatureExtractor\n",
    "from src.sampling.sampler import PolicySampler, SamplingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "PROJECT_ROOT = Path(PROJECT_PATH)\n",
    "DDS_PATH = PROJECT_ROOT / \"data\" / \"raw\" / \"dds_results\" / \"dds_results_100K_eval.npy\"\n",
    "PI_H_PATH = PROJECT_ROOT / \"checkpoints\" / \"pi_H\"\n",
    "PI_R_PATH = PROJECT_ROOT / \"checkpoints\" / \"pi_R\"\n",
    "\n",
    "# Verify paths exist\n",
    "print(f\"DDS exists: {DDS_PATH.exists()}\")\n",
    "print(f\"π^H exists: {PI_H_PATH.exists()}\")\n",
    "print(f\"π^R exists: {PI_R_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment\n",
    "print(\"Loading environment...\")\n",
    "env = BridgeBidding(dds_results_table_path=str(DDS_PATH))\n",
    "print(\"Environment loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load policies\n",
    "print(\"Loading π^H (Human proxy)...\")\n",
    "pi_H = PolicyWrapper(\n",
    "    PI_H_PATH,\n",
    "    model_type=\"DeepMind\",\n",
    "    activation=\"relu\",\n",
    "    model_file=\"model-sl.pkl\",\n",
    ")\n",
    "print(f\"π^H loaded: {pi_H}\")\n",
    "\n",
    "print(\"\\nLoading π^R (RL policy)...\")\n",
    "pi_R = PolicyWrapper(\n",
    "    PI_R_PATH,\n",
    "    model_type=\"DeepMind\",\n",
    "    activation=\"relu\",\n",
    "    model_file=\"model-pretrained-rl-with-fsp.pkl\",\n",
    ")\n",
    "print(f\"π^R loaded: {pi_R}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature extractor\n",
    "extractor = BridgeFeatureExtractor(normalize=False)\n",
    "print(f\"Feature extractor: {len(extractor.get_feature_names())} features\")\n",
    "print(f\"Features: {extractor.get_feature_names()[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fast Sampling with JIT\n",
    "\n",
    "Using a simplified JIT-compiled sampling loop for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from tqdm.auto import tqdm  # Auto picks notebook-friendly version\n",
    "\n",
    "# Configuration\n",
    "N_SAMPLES = 100_000\n",
    "SEED = 42\n",
    "SMOOTHING_EPSILON = 1e-5\n",
    "RUN_ID = \"100K_pi_H_v2\"\n",
    "OUTPUT_DIR = str(PROJECT_ROOT / \"data\" / \"processed\" / \"policy_samples\")\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  N_SAMPLES: {N_SAMPLES:,}\")\n",
    "print(f\"  SEED: {SEED}\")\n",
    "print(f\"  SMOOTHING_EPSILON: {SMOOTHING_EPSILON}\")\n",
    "print(f\"  RUN_ID: {RUN_ID}\")\n",
    "print(f\"  Action selection: π^H (realistic state distribution)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JIT compile env functions for speed\n",
    "env_init = jax.jit(env.init)\n",
    "env_step = jax.jit(env.step)\n",
    "\n",
    "# Get feature names for later\n",
    "FEATURE_NAMES = extractor.get_feature_names()\n",
    "N_FEATURES = len(FEATURE_NAMES)\n",
    "print(f\"Features: {N_FEATURES} total\")\n",
    "\n",
    "def fast_sample_jit(n_samples, seed=42):\n",
    "    \"\"\"Fast sampling using π^H for action selection.\"\"\"\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    \n",
    "    # Warmup JIT\n",
    "    print(\"Warming up JIT...\")\n",
    "    state = env_init(key)\n",
    "    for _ in range(10):\n",
    "        key, k1, k2 = jax.random.split(key, 3)\n",
    "        state = env_init(k1)\n",
    "        obs_f32 = state.observation.astype(jnp.float32)\n",
    "        mask = state.legal_action_mask\n",
    "        _ = pi_H.get_probs(obs_f32, mask)\n",
    "        _ = pi_R.get_probs(obs_f32, mask)\n",
    "        state = env_step(state, 0, k2)\n",
    "    print(\"JIT warmup complete!\")\n",
    "    \n",
    "    # Storage\n",
    "    all_obs, all_masks, all_pi_H, all_pi_R = [], [], [], []\n",
    "    all_ep_ids, all_timesteps, all_covariates = [], [], []\n",
    "    total, ep_id = 0, 0\n",
    "    \n",
    "    pbar = tqdm(total=n_samples, desc=\"Sampling\")\n",
    "    \n",
    "    while total < n_samples:\n",
    "        key, init_key = jax.random.split(key)\n",
    "        state = env_init(init_key)\n",
    "        timestep = 0\n",
    "        \n",
    "        while not state.terminated:\n",
    "            obs = state.observation\n",
    "            mask = state.legal_action_mask\n",
    "            obs_f32 = obs.astype(jnp.float32)\n",
    "            \n",
    "            # Get probabilities from both policies\n",
    "            probs_H, _ = pi_H.get_probs(obs_f32, mask)\n",
    "            probs_R, _ = pi_R.get_probs(obs_f32, mask)\n",
    "            \n",
    "            # Extract features using observation (not state)\n",
    "            feature_dict = extractor.extract(obs)\n",
    "            features = np.array([feature_dict[name] for name in FEATURE_NAMES], dtype=np.float32)\n",
    "            \n",
    "            # Store\n",
    "            all_obs.append(np.array(obs))\n",
    "            all_masks.append(np.array(mask))\n",
    "            all_pi_H.append(np.array(probs_H))\n",
    "            all_pi_R.append(np.array(probs_R))\n",
    "            all_ep_ids.append(ep_id)\n",
    "            all_timesteps.append(timestep)\n",
    "            all_covariates.append(features)\n",
    "            \n",
    "            # Select action using π^H (key change from random!)\n",
    "            key, act_key, step_key = jax.random.split(key, 3)\n",
    "            action = int(jax.random.categorical(act_key, jnp.log(probs_H + 1e-10)))\n",
    "            state = env_step(state, action, step_key)\n",
    "            \n",
    "            total += 1\n",
    "            timestep += 1\n",
    "            pbar.update(1)\n",
    "            \n",
    "            if total >= n_samples:\n",
    "                break\n",
    "        \n",
    "        ep_id += 1\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Apply smoothing: (p + ε) / (1 + K*ε)\n",
    "    K = 38  # number of actions\n",
    "    pi_H_arr = np.stack(all_pi_H[:n_samples])\n",
    "    pi_R_arr = np.stack(all_pi_R[:n_samples])\n",
    "    pi_H_smooth = (pi_H_arr + SMOOTHING_EPSILON) / (1 + K * SMOOTHING_EPSILON)\n",
    "    pi_R_smooth = (pi_R_arr + SMOOTHING_EPSILON) / (1 + K * SMOOTHING_EPSILON)\n",
    "    \n",
    "    return {\n",
    "        'observations': np.stack(all_obs[:n_samples]),\n",
    "        'legal_masks': np.stack(all_masks[:n_samples]),\n",
    "        'pi_H': pi_H_smooth.astype(np.float32),\n",
    "        'pi_R': pi_R_smooth.astype(np.float32),\n",
    "        'episode_ids': np.array(all_ep_ids[:n_samples], dtype=np.int32),\n",
    "        'timestep_in_episode': np.array(all_timesteps[:n_samples], dtype=np.int16),\n",
    "        'covariates': np.stack(all_covariates[:n_samples]),\n",
    "    }\n",
    "\n",
    "print(\"fast_sample_jit defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sampling!\n",
    "print(\"=\" * 60)\n",
    "print(f\"Starting sampling: {N_SAMPLES:,} samples\")\n",
    "print(f\"Action selection: π^H\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "samples = fast_sample_jit(N_SAMPLES, seed=SEED)\n",
    "\n",
    "print(f\"\\nSampling complete!\")\n",
    "print(f\"  Total samples: {len(samples['episode_ids']):,}\")\n",
    "print(f\"  Unique episodes: {len(np.unique(samples['episode_ids'])):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check - auction level distribution\n",
    "auction_level_idx = FEATURE_NAMES.index('auction_level')\n",
    "auction_levels = samples['covariates'][:, auction_level_idx]\n",
    "\n",
    "print(\"Auction level distribution:\")\n",
    "for level in range(8):\n",
    "    pct = np.mean(auction_levels == level) * 100\n",
    "    if pct > 0.1:\n",
    "        print(f\"  Level {level}: {pct:.1f}%\")\n",
    "\n",
    "# This should show most samples at levels 0-4, NOT mostly 7!\n",
    "if np.mean(auction_levels >= 6) > 0.3:\n",
    "    print(\"\\n⚠️ WARNING: Too many high-level contracts! Check action selection.\")\n",
    "else:\n",
    "    print(\"\\n✓ Auction distribution looks realistic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify samples\n",
    "print(\"Verifying samples...\")\n",
    "\n",
    "# Basic checks\n",
    "n_samples = len(samples['episode_ids'])\n",
    "n_episodes = len(np.unique(samples['episode_ids']))\n",
    "\n",
    "print(f\"\\n1. Sample counts:\")\n",
    "print(f\"   Total samples: {n_samples:,}\")\n",
    "print(f\"   Unique episodes: {n_episodes:,}\")\n",
    "print(f\"   Avg states/episode: {n_samples/n_episodes:.1f}\")\n",
    "\n",
    "print(f\"\\n2. Probability checks:\")\n",
    "# Check probabilities sum to 1\n",
    "pi_H_sums = np.sum(samples['pi_H'], axis=1)\n",
    "pi_R_sums = np.sum(samples['pi_R'], axis=1)\n",
    "print(f\"   π^H sum range: [{pi_H_sums.min():.6f}, {pi_H_sums.max():.6f}]\")\n",
    "print(f\"   π^R sum range: [{pi_R_sums.min():.6f}, {pi_R_sums.max():.6f}]\")\n",
    "\n",
    "# Check min probability (should be ~SMOOTHING_EPSILON due to smoothing)\n",
    "min_prob_H = samples['pi_H'].min()\n",
    "min_prob_R = samples['pi_R'].min()\n",
    "print(f\"   π^H min prob: {min_prob_H:.2e} (expected ~{SMOOTHING_EPSILON:.0e})\")\n",
    "print(f\"   π^R min prob: {min_prob_R:.2e} (expected ~{SMOOTHING_EPSILON:.0e})\")\n",
    "\n",
    "print(f\"\\n3. Legal mask consistency:\")\n",
    "# Check that legal actions have reasonable probabilities\n",
    "legal_probs_H = samples['pi_H'][samples['legal_masks']]\n",
    "illegal_probs_H = samples['pi_H'][~samples['legal_masks']]\n",
    "print(f\"   Legal action prob range: [{legal_probs_H.min():.6f}, {legal_probs_H.max():.6f}]\")\n",
    "print(f\"   Illegal action prob range: [{illegal_probs_H.min():.6f}, {illegal_probs_H.max():.6f}]\")\n",
    "\n",
    "print(f\"\\n4. Shape verification:\")\n",
    "for key, arr in samples.items():\n",
    "    print(f\"   {key}: {arr.shape} ({arr.dtype})\")\n",
    "\n",
    "print(\"\\n✓ Verification complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save samples\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Save .npz\n",
    "npz_path = os.path.join(OUTPUT_DIR, f\"{RUN_ID}_policy_samples.npz\")\n",
    "np.savez_compressed(npz_path, **samples)\n",
    "print(f\"Saved samples to: {npz_path}\")\n",
    "\n",
    "# Create and save metadata\n",
    "action_names = [\"Pass\", \"Dbl\", \"Rdbl\", \n",
    "                \"1C\", \"1D\", \"1H\", \"1S\", \"1NT\",\n",
    "                \"2C\", \"2D\", \"2H\", \"2S\", \"2NT\",\n",
    "                \"3C\", \"3D\", \"3H\", \"3S\", \"3NT\",\n",
    "                \"4C\", \"4D\", \"4H\", \"4S\", \"4NT\",\n",
    "                \"5C\", \"5D\", \"5H\", \"5S\", \"5NT\",\n",
    "                \"6C\", \"6D\", \"6H\", \"6S\", \"6NT\",\n",
    "                \"7C\", \"7D\", \"7H\", \"7S\", \"7NT\"]\n",
    "\n",
    "# Compute per-action legal rates\n",
    "legal_rates = np.mean(samples['legal_masks'].astype(float), axis=0).tolist()\n",
    "\n",
    "# Find rare actions (< 0.5% legal rate)\n",
    "rare_actions = [action_names[i] for i, rate in enumerate(legal_rates) if rate < 0.005]\n",
    "\n",
    "# Episode statistics\n",
    "ep_counts = np.bincount(samples['episode_ids'])\n",
    "states_per_episode = {\n",
    "    'min': int(ep_counts.min()),\n",
    "    'max': int(ep_counts.max()),\n",
    "    'median': float(np.median(ep_counts)),\n",
    "    'mean': float(np.mean(ep_counts)),\n",
    "}\n",
    "\n",
    "metadata = {\n",
    "    'n_samples': int(n_samples),\n",
    "    'n_episodes': int(n_episodes),\n",
    "    'feature_names': FEATURE_NAMES,  # Use the variable we defined\n",
    "    'action_names': action_names,\n",
    "    'ref_action': 'Pass',\n",
    "    'ref_action_idx': 0,\n",
    "    'action_legal_rates': legal_rates,\n",
    "    'rare_actions': rare_actions,\n",
    "    'states_per_episode': states_per_episode,\n",
    "    'sampling_config': {\n",
    "        'seed': SEED,\n",
    "        'smoothing_epsilon': SMOOTHING_EPSILON,\n",
    "        'behavior_policy': 'pi_H',\n",
    "    }\n",
    "}\n",
    "\n",
    "json_path = os.path.join(OUTPUT_DIR, f\"{RUN_ID}_metadata.json\")\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"Saved metadata to: {json_path}\")\n",
    "\n",
    "print(f\"\\n✓ All files saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"Sample Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nπ^H statistics:\")\n",
    "entropy_H = -np.mean(np.sum(samples['pi_H'] * np.log(samples['pi_H'] + 1e-10), axis=1))\n",
    "print(f\"  Mean entropy: {entropy_H:.3f}\")\n",
    "print(f\"  Max prob range: [{np.min(np.max(samples['pi_H'], axis=1)):.3f}, {np.max(np.max(samples['pi_H'], axis=1)):.3f}]\")\n",
    "\n",
    "print(f\"\\nπ^R statistics:\")\n",
    "entropy_R = -np.mean(np.sum(samples['pi_R'] * np.log(samples['pi_R'] + 1e-10), axis=1))\n",
    "print(f\"  Mean entropy: {entropy_R:.3f}\")\n",
    "print(f\"  Max prob range: [{np.min(np.max(samples['pi_R'], axis=1)):.3f}, {np.max(np.max(samples['pi_R'], axis=1)):.3f}]\")\n",
    "\n",
    "print(f\"\\nAction legal rates:\")\n",
    "for i, name in enumerate(action_names[:10]):\n",
    "    print(f\"  {name}: {legal_rates[i]*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nCovariate summary (first 5 features):\")\n",
    "for i, name in enumerate(FEATURE_NAMES[:5]):\n",
    "    values = samples['covariates'][:, i]\n",
    "    print(f\"  {name}: mean={np.mean(values):.2f}, std={np.std(values):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Download (Optional)\n",
    "\n",
    "If you want to download the file to your local machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Download to local machine\n",
    "# from google.colab import files\n",
    "# files.download(str(output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "The policy samples are saved to:\n",
    "```\n",
    "data/processed/policy_samples/<run_id>_policy_samples.npz\n",
    "data/processed/policy_samples/<run_id>_metadata.json\n",
    "```\n",
    "\n",
    "### Output Arrays\n",
    "| Array | Shape | Dtype | Description |\n",
    "|-------|-------|-------|-------------|\n",
    "| observations | (N, 480) | bool | Raw PGX observations |\n",
    "| covariates | (N, 48) | float32 | 48 statistical features |\n",
    "| pi_H | (N, 38) | float32 | Smoothed π^H probabilities |\n",
    "| pi_R | (N, 38) | float32 | Smoothed π^R probabilities |\n",
    "| legal_masks | (N, 38) | bool | Legal action masks |\n",
    "| episode_ids | (N,) | int32 | Episode index (for cluster bootstrap) |\n",
    "| board_ids | (N,) | int32 | Unique board index |\n",
    "| timestep_in_episode | (N,) | int16 | Bidding round in episode |\n",
    "\n",
    "### Critical Metadata\n",
    "- `ref_action_idx = 0` (Pass is always action 0 in PGX)\n",
    "- `action_names = [\"Pass\", \"Dbl\", \"Rdbl\", \"1C\", ..., \"7NT\"]`\n",
    "\n",
    "Next step: Use these samples for FDA analysis in Step 4."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
